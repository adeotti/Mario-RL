{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":365364,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":303051,"modelId":323568},{"sourceId":368123,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":304981,"modelId":325426}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ad7f898a","cell_type":"code","source":"from IPython.display import clear_output\n!pip install numpy==1.25.2\n!pip install torch==2.7.0\n!pip install torchvision==0.22.0\n!pip install gym_super_mario_bros==7.4.0\n!pip install gym==0.26.2\n!pip install nes_py==8.2.1\n!pip install gymnasium==0.29.1\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:53:59.001478Z","iopub.execute_input":"2025-05-01T13:53:59.001722Z","iopub.status.idle":"2025-05-01T13:57:53.081365Z","shell.execute_reply.started":"2025-05-01T13:53:59.001693Z","shell.execute_reply":"2025-05-01T13:57:53.080461Z"}},"outputs":[],"execution_count":1},{"id":"f3530fc0","cell_type":"code","source":"import random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport gym_super_mario_bros\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom gymnasium.wrappers import FrameStack\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torchvision.transforms import v2,Resize\nfrom torchvision.transforms.functional import to_tensor\n\n# hypers \nlr = 0.0003\n_lambda_ = 0.99\ngamma = 0.99\nepsilon = 0.2\nc1 = 0.5\nnumFrames = 5 \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nenv = gym_super_mario_bros.make('SuperMarioBros-v0',apply_api_compatibility=True)\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nenv = FrameStack(env,numFrames)\n\ndef transform_env_output(observation): # -> torch.Size([1, 5, 150, 150])\n    _list_ = []\n    for element in observation:\n        tonumpy = np.array(element)\n        topil = Image.fromarray(tonumpy)\n        gray = to_tensor(v2.Grayscale(1)(topil))\n        resized = Resize((150,150))(gray)\n        _list_.append(resized)\n    return torch.stack(_list_,dim=0).permute(1,0,2,3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:57:53.082956Z","iopub.execute_input":"2025-05-01T13:57:53.083190Z","iopub.status.idle":"2025-05-01T13:57:57.357717Z","shell.execute_reply.started":"2025-05-01T13:57:53.083171Z","shell.execute_reply":"2025-05-01T13:57:57.356867Z"}},"outputs":[],"execution_count":2},{"id":"5d5ed85b","cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\n\nclass network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.input = nn.LazyConv2d(1,(1,1))\n        self.conv1 = nn.LazyConv2d(1,(3,3),stride=2)\n        self.conv2 = nn.LazyConv2d(1,(3,3),stride=1)\n        self.conv3 = nn.LazyConv2d(1,(3,3),stride=2)\n        \n        self.linear1 = nn.LazyLinear(3000)\n        self.linear2 = nn.LazyLinear(1500)\n        self.linear3 = nn.LazyLinear(750)\n        self.linear4 = nn.LazyLinear(375)\n\n        self.policyHead = nn.LazyLinear(7)\n        self.valueHead = nn.LazyLinear(1)\n\n        self.optim = Adam(self.parameters(),lr=lr)\n        \n    def forward(self,x):\n        x = F.relu(self.input(x))\n        x = self.conv1(x)\n        x = F.relu(self.conv2(x))\n        x = self.conv3(x)\n        x = F.relu(torch.flatten(x,start_dim=1))\n        x = self.linear1(x)\n        x = F.relu(self.linear2(x))\n        x = self.linear3(x)\n        x = F.relu(self.linear4(x))\n        policyOut = self.policyHead(x)\n        valueOut = self.valueHead(x)\n        return F.softmax(policyOut,-1),valueOut\n\nmodel = network() \nmodel.forward(torch.rand((1,5,150,150),dtype=torch.float))\nmodel.to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/mario800k/pytorch/default/1/mario800\"),strict=False)\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:16:52.193883Z","iopub.execute_input":"2025-05-01T14:16:52.194185Z","iopub.status.idle":"2025-05-01T14:16:52.322967Z","shell.execute_reply.started":"2025-05-01T14:16:52.194162Z","shell.execute_reply":"2025-05-01T14:16:52.322328Z"}},"outputs":[],"execution_count":8},{"id":"03f04ffd","cell_type":"code","source":"from torch.distributions import Categorical\nimport gc\n\nclass Memory:\n    def __init__(self):\n        self.network = model\n        self.env = env\n        self.gamma = gamma\n        self._lambda_ = _lambda_\n        self.data = []\n        self.pointer = 0\n    \n    def rollout(self,batchsize):\n        self.clear()\n        stacked_frames,info = self.env.reset()\n        with torch.no_grad(): \n            for _ in range(batchsize):  \n                tranformed_observation = transform_env_output(stacked_frames).to(device)\n                policy_output , value = self.network.forward(tranformed_observation)\n                distribution = Categorical(policy_output)\n                action = distribution.sample()\n                prob = _distribution.log_prob(action)\n                state,reward,done,_,_ = self.env.step(action.item())\n                if done:\n                    stacked_frames,info = self.env.reset() # reset env if done to keep training\n                self.data.append([state,torch.tensor(reward),value,prob,action,done])\n\n        _,rewards,values,_,_,_ = zip(*self.data) # compute advantages \n        _rewards = torch.stack(rewards).to(device)\n        _values = torch.stack(values).reshape(batchsize).to(device)\n        _values = torch.cat((_values,torch.tensor([0],device=device)))\n        \n        n = torch.arange(batchsize,device=device)\n        temporalDifferences  = _rewards[n] + self.gamma*_values[n+1] - _values[n]\n        temporalDifferences = torch.flip(temporalDifferences,dims=[-1])\n        _advantage = 0\n        advantages = temporalDifferences[n] + (self._lambda_ * self.gamma * _advantage)\n        advantages = torch.flip(advantages,dims=[-1]) \n      \n        for data,item in zip(self.data,advantages): # append advantages to data\n            data.append(item)\n        random.shuffle(self.data) \n\n    def sample(self,number):\n        output = self.data[self.pointer:self.pointer+number]\n        self.pointer+=number\n        states,rewards,values,logProb,actions,done,advantages = zip(*output)\n        return states,actions,rewards,values,logProb,advantages,done\n\n    def clear(self):\n        self.data = []\n        self.pointer = 0\n        gc.collect()\n        torch.cuda.empty_cache()\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:17:02.440366Z","iopub.execute_input":"2025-05-01T14:17:02.440643Z","iopub.status.idle":"2025-05-01T14:17:02.450875Z","shell.execute_reply.started":"2025-05-01T14:17:02.440622Z","shell.execute_reply":"2025-05-01T14:17:02.450125Z"}},"outputs":[],"execution_count":10},{"id":"d0608a24","cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nclass agent:\n    def __init__(self):\n        self.env = env\n        self.memory = Memory()\n        self.network = model\n        self.epochs = 2_000\n        self.batchsize = 2_700\n        self.minibatch = 450\n        self.optim_step = 10\n        self.writter = SummaryWriter(\"/kaggle/working/\")\n\n    def save(self,k):\n        torch.save(self.network.state_dict(),f\"/kaggle/working/mario{k}\")\n  \n    def train(self):\n        for traj in tqdm(range(self.epochs),total=self.epochs):\n            self.memory.rollout(self.batchsize)\n            for n in range(self.batchsize//self.minibatch): # sample minibatches\n                states,actions,rewards,values,old_log_prob,advantages,done = self.memory.sample(self.minibatch)    \n                for m in range(self.optim_step): # many passes on minibatches\n                    _advantages = torch.stack(advantages)\n                    _values = torch.stack(values,dim=-1)\n                    _old_log_prob = torch.stack(old_log_prob,dim=1)\n                    _rewards = torch.mean(torch.stack(rewards)) \n                    \n                    vtarget = _advantages + _values\n                    loss_critic = F.mse_loss(_values,vtarget)\n                   \n                    states_list = []\n                    for element in states:  \n                        transformed_states = transform_env_output(element).to(device)\n                        states_list.append(transformed_states)\n                    stacked_states = torch.stack(states_list,dim=0).squeeze(1) # ->  torch.Size([minibatch, 5, 150, 150])\n                    stacked_actions = torch.stack(actions,dim=1) \n                    \n                    policy_output,_ = self.network.forward(stacked_states)\n                    dist = Categorical(policy_output)\n                    new_log_prob = dist.log_prob(stacked_actions)\n                    \n                    ratio = torch.exp(new_log_prob)/torch.exp(_old_log_prob)\n    \n                    loss_policy_list = [] \n                    for i in range(len(advantages)):\n                        ratio_advantages = ratio*_advantages[i]\n                        clipped_ratio_advantages = torch.clamp(ratio_advantages,(1-epsilon),(1+epsilon))*_advantages[i]\n                        loss = torch.min(ratio_advantages,clipped_ratio_advantages)\n                        loss_policy_list.append(loss)\n    \n                    loss_policy = -torch.mean(torch.stack(loss_policy_list))\n                    totalLoss = loss_policy + c1*loss_critic\n                    self.network.optim.zero_grad()\n                    totalLoss.backward(retain_graph=True)\n                    self.network.optim.step()\n    \n            self.writter.add_scalar(\"main/Reward\",_rewards)\n            self.writter.add_scalar(\"main/Loss\",totalLoss)\n            \n            if traj % 100 == 0 : # save every 200k steps\n                self.save(traj)\n\n        self.save(\"FULLYTRAINED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:19:28.930925Z","iopub.execute_input":"2025-05-01T14:19:28.931785Z","iopub.status.idle":"2025-05-01T14:19:28.943219Z","shell.execute_reply.started":"2025-05-01T14:19:28.931752Z","shell.execute_reply":"2025-05-01T14:19:28.942513Z"}},"outputs":[],"execution_count":14},{"id":"debf643d-811f-452e-b50f-659ec8710407","cell_type":"code","source":"agent().train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T14:19:33.262799Z","iopub.execute_input":"2025-05-01T14:19:33.263440Z"}},"outputs":[{"name":"stderr","text":"  1%|▏         | 26/2000 [1:35:24<120:34:22, 219.89s/it]","output_type":"stream"}],"execution_count":null}]}