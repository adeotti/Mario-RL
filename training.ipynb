{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f898a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T13:53:59.001722Z",
     "iopub.status.busy": "2025-05-01T13:53:59.001478Z",
     "iopub.status.idle": "2025-05-01T13:57:53.081365Z",
     "shell.execute_reply": "2025-05-01T13:57:53.080461Z",
     "shell.execute_reply.started": "2025-05-01T13:53:59.001693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Requirements\n",
    "from IPython.display import clear_output\n",
    "!pip install numpy==1.25.2\n",
    "!pip install torch==2.7.0\n",
    "!pip install torchvision==0.22.0\n",
    "!pip install gym_super_mario_bros==7.4.0\n",
    "!pip install gym==0.26.2\n",
    "!pip install nes_py==8.2.1\n",
    "!pip install gymnasium==0.29.1\n",
    "clear_output()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3530fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T13:57:53.083190Z",
     "iopub.status.busy": "2025-05-01T13:57:53.082956Z",
     "iopub.status.idle": "2025-05-01T13:57:57.357717Z",
     "shell.execute_reply": "2025-05-01T13:57:57.356867Z",
     "shell.execute_reply.started": "2025-05-01T13:57:53.083171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gymnasium.wrappers import FrameStack\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2,Resize\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# hypers \n",
    "lr = 0.0003\n",
    "_lambda_ = 0.99\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "c1 = 0.5\n",
    "numFrames = 5 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0',apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = FrameStack(env,numFrames)\n",
    "\n",
    "def transform_env_output(observation): # -> torch.Size([1, 5, 150, 150])\n",
    "    _list_ = []\n",
    "    for element in observation:\n",
    "        tonumpy = np.array(element)\n",
    "        topil = Image.fromarray(tonumpy)\n",
    "        gray = to_tensor(v2.Grayscale(1)(topil))\n",
    "        resized = Resize((150,150))(gray)\n",
    "        _list_.append(resized)\n",
    "    return torch.stack(_list_,dim=0).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ed85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T14:16:52.194185Z",
     "iopub.status.busy": "2025-05-01T14:16:52.193883Z",
     "iopub.status.idle": "2025-05-01T14:16:52.322967Z",
     "shell.execute_reply": "2025-05-01T14:16:52.322328Z",
     "shell.execute_reply.started": "2025-05-01T14:16:52.194162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = nn.LazyConv2d(1,(1,1))\n",
    "        self.conv1 = nn.LazyConv2d(1,(3,3),stride=2)\n",
    "        self.conv2 = nn.LazyConv2d(1,(3,3),stride=1)\n",
    "        self.conv3 = nn.LazyConv2d(1,(3,3),stride=2)\n",
    "        \n",
    "        self.linear1 = nn.LazyLinear(3000)\n",
    "        self.linear2 = nn.LazyLinear(1500)\n",
    "        self.linear3 = nn.LazyLinear(750)\n",
    "        self.linear4 = nn.LazyLinear(375)\n",
    "\n",
    "        self.policyHead = nn.LazyLinear(7)\n",
    "        self.valueHead = nn.LazyLinear(1)\n",
    "\n",
    "        self.optim = Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(torch.flatten(x,start_dim=1))\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(self.linear4(x))\n",
    "        policyOut = self.policyHead(x)\n",
    "        valueOut = self.valueHead(x)\n",
    "        return F.softmax(policyOut,-1),valueOut\n",
    "\n",
    "model = network() \n",
    "model.forward(torch.rand((1,5,150,150),dtype=torch.float))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"./policies/mario800\",map_location=device),strict=False)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f04ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T14:17:02.440643Z",
     "iopub.status.busy": "2025-05-01T14:17:02.440366Z",
     "iopub.status.idle": "2025-05-01T14:17:02.450875Z",
     "shell.execute_reply": "2025-05-01T14:17:02.450125Z",
     "shell.execute_reply.started": "2025-05-01T14:17:02.440622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "import gc\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.network = model\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self._lambda_ = _lambda_\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def rollout(self,batchsize):\n",
    "        self.clear()\n",
    "        stacked_frames,info = self.env.reset()\n",
    "        with torch.no_grad(): \n",
    "            for _ in range(batchsize):  \n",
    "                tranformed_observation = transform_env_output(stacked_frames).to(device)\n",
    "                policy_output , value = self.network.forward(tranformed_observation)\n",
    "                distribution = Categorical(policy_output)\n",
    "                action = distribution.sample()\n",
    "                prob = distribution.log_prob(action)\n",
    "                state,reward,done,_,_ = self.env.step(action.item())\n",
    "                if done:\n",
    "                    stacked_frames,info = self.env.reset() # reset env if done to keep training\n",
    "                self.data.append([state,torch.tensor(reward),value,prob,action,done])\n",
    "\n",
    "        _,rewards,values,_,_,_ = zip(*self.data) # compute advantages \n",
    "        _rewards = torch.stack(rewards).to(device)\n",
    "        _values = torch.stack(values).reshape(batchsize).to(device)\n",
    "        _values = torch.cat((_values,torch.tensor([0],device=device)))\n",
    "        \n",
    "        n = torch.arange(batchsize,device=device)\n",
    "        temporalDifferences  = _rewards[n] + self.gamma*_values[n+1] - _values[n]\n",
    "        temporalDifferences = torch.flip(temporalDifferences,dims=[-1])\n",
    "        _advantage = 0\n",
    "        advantages = temporalDifferences[n] + (self._lambda_ * self.gamma * _advantage)\n",
    "        advantages = torch.flip(advantages,dims=[-1]) \n",
    "      \n",
    "        for data,item in zip(self.data,advantages): # append advantages to data\n",
    "            data.append(item)\n",
    "        random.shuffle(self.data) \n",
    "\n",
    "    def sample(self,number):\n",
    "        output = self.data[self.pointer:self.pointer+number]\n",
    "        self.pointer+=number\n",
    "        states,rewards,values,logProb,actions,done,advantages = zip(*output)\n",
    "        return states,actions,rewards,values,logProb,advantages,done\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pointer = 0\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0608a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T14:19:28.931785Z",
     "iopub.status.busy": "2025-05-01T14:19:28.930925Z",
     "iopub.status.idle": "2025-05-01T14:19:28.943219Z",
     "shell.execute_reply": "2025-05-01T14:19:28.942513Z",
     "shell.execute_reply.started": "2025-05-01T14:19:28.931752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class agent:\n",
    "    def __init__(self):\n",
    "        self.env = env\n",
    "        self.memory = Memory()\n",
    "        self.network = model\n",
    "        self.epochs = 2_000\n",
    "        self.batchsize = 2_700\n",
    "        self.minibatch = 450\n",
    "        self.optim_step = 10\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "\n",
    "    def save(self,k):\n",
    "        torch.save(self.network.state_dict(),f\"./mario{k}\")\n",
    "  \n",
    "    def train(self):\n",
    "        for traj in tqdm(range(self.epochs),total=self.epochs):\n",
    "            self.memory.rollout(self.batchsize)\n",
    "\n",
    "            for n in range(self.batchsize//self.minibatch):\n",
    "                states,actions,rewards,values,old_log_prob,advantages,done = self.memory.sample(self.minibatch)    \n",
    "\n",
    "                for m in range(self.optim_step): # many passes on minibatches\n",
    "                    _advantages = torch.stack(advantages)\n",
    "                    _values = torch.stack(values,dim=-1)\n",
    "                    _old_log_prob = torch.stack(old_log_prob,dim=1)\n",
    "                    _rewards = torch.mean(torch.stack(rewards)) \n",
    "                    \n",
    "                    vtarget = _advantages + _values\n",
    "                    loss_critic = F.mse_loss(_values,vtarget)\n",
    "                   \n",
    "                    states_list = []\n",
    "                    for element in states:  \n",
    "                        transformed_states = transform_env_output(element).to(device)\n",
    "                        states_list.append(transformed_states)\n",
    "                    stacked_states = torch.stack(states_list,dim=0).squeeze(1) # ->  torch.Size([minibatch, 5, 150, 150])\n",
    "                    stacked_actions = torch.stack(actions,dim=1) \n",
    "                    \n",
    "                    policy_output,_ = self.network.forward(stacked_states)\n",
    "                    dist = Categorical(policy_output)\n",
    "                    new_log_prob = dist.log_prob(stacked_actions)\n",
    "                    \n",
    "                    ratio = torch.exp(new_log_prob)/torch.exp(_old_log_prob)\n",
    "    \n",
    "                    loss_policy_list = [] \n",
    "                    for i in range(len(advantages)):\n",
    "                        ratio_advantages = ratio*_advantages[i]\n",
    "                        clipped_ratio_advantages = torch.clamp(ratio_advantages,(1-epsilon),(1+epsilon))*_advantages[i]\n",
    "                        loss = torch.min(ratio_advantages,clipped_ratio_advantages)\n",
    "                        loss_policy_list.append(loss)\n",
    "    \n",
    "                    loss_policy = -torch.mean(torch.stack(loss_policy_list))\n",
    "                    totalLoss = loss_policy + c1*loss_critic\n",
    "                    self.network.optim.zero_grad()\n",
    "                    totalLoss.backward(retain_graph=True)\n",
    "                    self.network.optim.step()\n",
    "    \n",
    "            self.writter.add_scalar(\"main/Reward\",_rewards)\n",
    "            self.writter.add_scalar(\"main/Loss\",totalLoss)\n",
    "            \n",
    "            if traj % 100 == 0 : # save every 270k steps\n",
    "                self.save(traj)\n",
    "\n",
    "        self.save(\"_fully_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf643d-811f-452e-b50f-659ec8710407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T14:19:33.263440Z",
     "iopub.status.busy": "2025-05-01T14:19:33.262799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "agent().train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 323568,
     "modelInstanceId": 303051,
     "sourceId": 365364,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 325426,
     "modelInstanceId": 304981,
     "sourceId": 368123,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
